{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning @ UDD\n",
    "### Instructor: Visiting Professor Rossano Schifanella\n",
    "\n",
    "### References:\n",
    "* **scikit-learn tutorial** at [SciPy 2017](https://github.com/amueller/scipy-2017-sklearn) by [Alexandre Gramfort](http://http://alexandre.gramfort.net/)  [@agramfort](https://twitter.com/agramfort) (Inria, Université Paris-Saclay) and [Andreas Mueller](http://amuller.github.io) [@amuellerml](https://twitter.com/amuellerml) (Columbia University). See the book [Introduction to Machine Learning with Python](http://shop.oreilly.com/product/0636920030515.do) for more details. \n",
    "* **Data Mining, Statistical Modeling and Machine Learning** class by Dr. Ciro Cattuto, Dr. Laetitia Gauvin, Dr. André Panisson (ISI Foundation, Turi, Italy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install requires python libraries\n",
    "\n",
    "We use pip but the same works for \"conda\".\n",
    "\n",
    ">pip install scipy\n",
    "\n",
    ">pip install pandas\n",
    "\n",
    ">pip install scikit-learn\n",
    "\n",
    "optional: \n",
    "\n",
    ">pip install seaborn\n",
    "\n",
    "Mac users: please try installing graphviz using homebrew instead of anaconda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to need a lot of Python packages, so let's start by importing all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from IPython.display import Image\n",
    "import graphviz \n",
    "\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to do a lot of repetitive stuff, so let's predefine some useful functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that gives a visual representation of the decision tree\n",
    "def show_decision_tree(model):\n",
    "    dot_data = tree.export_graphviz(decision_tree, out_file=None) \n",
    "    graph = graphviz.Source(dot_data) \n",
    "#     To save on a PDF file\n",
    "#     graph.render(\"iris\")\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iris.keys())\n",
    "print(iris.target_names)\n",
    "print(iris.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful features\n",
    "Let's take a look at one of our features -- `\"petal length (cm)\"`. Is this feature useful? Let's plot the possible values of `\"petal_length\"` and color code our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_index = iris.feature_names.index('petal length (cm)')\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "for label, color in zip(range(len(iris.target_names)), colors):\n",
    "    plt.hist(iris.data[iris.target==label, feature_index], \n",
    "             label=iris.target_names[label],\n",
    "             color=color)\n",
    "plt.xlabel(iris.feature_names[feature_index])\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is `\"petal_length\"` actually useful? Let's quantify it.\n",
    "\n",
    "**Entropy** ($H$) and **information gain** ($IG$) are crucial in determining which features are the most informative. Given the data, it is fairly straight forward to calculate both of these.\n",
    "\n",
    "<table style=\"border: 0px\">\n",
    "<tr style=\"border: 0px;background:'white'\"\">\n",
    "<td style=\"border: 0px;background:'white'\"\"><img src=\"images/dsfb_0304.png\" height=80% width=80%>\n",
    "Figure 3-4. Splitting the \"write-off\" sample into two segments, based on splitting the Balance attribute (account balance) at 50K.</td>\n",
    "<td style=\"border: 0px; width: 30px\"></td>\n",
    "<td style=\"border: 0px\"><img src=\"images/dsfb_0305.png\" height=75% width=75%>\n",
    "Figure 3-5. A classification tree split on the three-values Residence attribute.</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(target):\n",
    "    '''\n",
    "    Formula: entropy = -p1*log(p1) - p2*log(p2) - ...\n",
    "    '''\n",
    "    # Get the number of users\n",
    "    n = len(target)\n",
    "    # Count how frequently each unique value occurs\n",
    "    un,counts=np.unique(target,return_counts=True)\n",
    "    # Initialize entropy\n",
    "    entropy = 0\n",
    "    # If the split is perfect, return 0\n",
    "    if len(counts) <= 1 or 0 in counts:\n",
    "        return entropy\n",
    "    # Otherwise, for each possible value, update entropy\n",
    "    for count in counts:\n",
    "        entropy += math.log(float(count)/n, len(counts)) * count/n\n",
    "    # Return entropy\n",
    "    return -1 * entropy\n",
    "\n",
    "def information_gain(feature, target, threshold):\n",
    "    '''\n",
    "    Formula: IG(parent,children)=entropy(parent) - [p(c1)*entropy(c1) + p(c2)*entropy(c2) + ...]\n",
    "    '''\n",
    "    # Dealing with numpy arrays makes this slightly easier\n",
    "    target = np.array(target)\n",
    "    feature = np.array(feature)\n",
    "    # Cut the feature vector on the threshold\n",
    "    feature = (feature < threshold)\n",
    "    # Initialize information gain with the parent entropy\n",
    "    ig = entropy(target)\n",
    "    # For both sides of the threshold, update information gain\n",
    "    for level, count in zip([0, 1], np.bincount(feature).astype(float)):\n",
    "        ig -= count/len(feature) * entropy(target[feature == level])\n",
    "    # Return information gain\n",
    "    return ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way of calculating $H$ and $IG$, let's pick a threshold, split `\"petal_length\"`, and calculate $IG$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X[:,2])\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "threshold = 4.7\n",
    "print(\"IG = %.4f with a thresholding of %.2f\" %(information_gain(X[:, 2], y,threshold), threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be more precise, we can iterate through all values and find the best split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def best_threshold(X, y):\n",
    "    maximum_ig = 0\n",
    "    maximum_threshold = 0\n",
    "\n",
    "    for threshold in X:\n",
    "        ig = information_gain(X, y, threshold)\n",
    "        if ig > maximum_ig:\n",
    "            maximum_ig = ig\n",
    "            maximum_threshold = threshold\n",
    "\n",
    "    return \"The maximum IG = %.3f and it occured by splitting on %.4f.\" % (maximum_ig, maximum_threshold)\n",
    "\n",
    "print(best_threshold(X[:, 2], y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can do this with just sklearn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(max_depth=1, criterion=\"gini\")\n",
    "decision_tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_decision_tree(decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use this as our decision tree, how accurate is it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy = %.3f\" % (decision_tree.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add one more level to our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(max_depth=5, criterion=\"entropy\")\n",
    "decision_tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_decision_tree(decision_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy = %.3f\" % (decision_tree.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "1. Compare the best thresholds for different attributes.\n",
    "2. Plot on the ``petal_length`` vs ``petal_width`` figure the lines corresponding to the best thresholds and that allow to classify the points\n",
    "3. Calculate the accuracy for each one level decision tree, obtained for the different attributes\n",
    "4. Now take the dataset, take randomly 2/3 of rows to create a training dataset, and the rest for the test\n",
    "5. Redo the decision tree and calculate the accuracy for the training set and test set. Do you get the same decision tree as with using the whole dataset?\n",
    "6. Starting from the function `best_threshold`, create a function that returns the IG for different thredholds, not simply the best one. Plot IG vs the threshold. This is always a good practice to understand how good your best threshold is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets for both X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split our data, let's check how well a model does when it is fit on a \"training\" set and then used to predict on both the training set as well as our hold out set. Remember, the model has never seen this hold out \"test\" set before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "model = DecisionTreeClassifier(max_depth=2)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on training = %.3f\" % model.score(X_train, y_train))\n",
    "print(\"Accuracy on test = %.3f\" % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_train = []\n",
    "accuracies_test = []\n",
    "depths = range(1, 9)\n",
    "\n",
    "for md in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=md)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    accuracies_train.append(model.score(X_train, y_train))\n",
    "    accuracies_test.append(model.score(X_test, y_test))\n",
    "\n",
    "plt.plot(depths, accuracies_train, label=\"Train\")\n",
    "plt.plot(depths, accuracies_test, label=\"Test\")\n",
    "plt.title(\"Performance on train and test data\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim([0.85, 1.05])\n",
    "plt.xlim([1,9])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iris actually works very well, but let's see what happens when we use some artificial data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details on model evaluation in the next class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips on pratical use\n",
    "\n",
    "* Decision trees tend to overfit on data with a large number of features. Getting the right ratio of samples to number of features is important, since a tree with few samples in high dimensional space is very likely to overfit.\n",
    "\n",
    "* Consider performing dimensionality reduction (PCA, ICA, or Feature selection) beforehand to give your tree a better chance of finding features that are discriminative.\n",
    "\n",
    "* Visualise your tree as you are training by using the export function. Use *max\\_depth=3* as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth.\n",
    "\n",
    "* Remember that the number of samples required to populate the tree doubles for each additional level the tree grows to. Use max_depth to control the size of the tree to prevent overfitting.\n",
    "\n",
    "* Use min_samples_split or min_samples_leaf to control the number of samples at a leaf node. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try min_samples_leaf=5 as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. The main difference between the two is that min_samples_leaf guarantees a minimum number of samples in a leaf, while min_samples_split can create arbitrary small leaves, though min_samples_split is more common in the literature.\n",
    "\n",
    "* Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (sample_weight) for each class to the same value. Also note that weight-based pre-pruning criteria, such as min_weight_fraction_leaf, will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like min_samples_leaf.\n",
    "\n",
    "* If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as min_weight_fraction_leaf, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights.\n",
    "\n",
    "* All decision trees use np.float32 arrays internally. If training data is not in this format, a copy of the dataset will be made.\n",
    "\n",
    "* If the input matrix X is very sparse, it is recommended to convert to sparse csc_matrix before calling fit and sparse csr_matrix before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the decision surface of a decision tree on the iris dataset\n",
    "\n",
    "Plot the decision surface of a decision tree trained on pairs\n",
    "of features of the iris dataset.\n",
    "\n",
    "See `decision tree <tree>` for more information on the estimator.\n",
    "\n",
    "For each pair of iris features, the decision tree learns decision\n",
    "boundaries made of combinations of simple thresholding rules inferred from\n",
    "the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_classes = 3\n",
    "plot_colors = \"ryb\"\n",
    "plot_step = 0.02\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "\n",
    "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
    "                                [1, 2], [1, 3], [2, 3]]):\n",
    "    # We only take the two corresponding features\n",
    "    X = iris.data[:, pair]\n",
    "    y = iris.target\n",
    "\n",
    "    # Train\n",
    "    clf = DecisionTreeClassifier(max_depth=5)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    plt.subplot(2, 3, pairidx + 1)\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "    plt.xlabel(iris.feature_names[pair[0]])\n",
    "    plt.ylabel(iris.feature_names[pair[1]])\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, color in zip(range(n_classes), plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
    "                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "plt.suptitle(\"Decision surface of a decision tree using paired features\")\n",
    "plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
